#!/usr/bin/env python3
"""
è´£ä»»ç¼–è¾‘å·¥å…· - å‡ºç‰ˆçº§è´¨é‡æ£€æŸ¥

æ‰§è¡Œè´£ä»»ç¼–è¾‘çš„ä¸‰å®¡ä¸‰æ ¡æ£€æŸ¥ï¼Œç¡®ä¿å‡ºç‰ˆè´¨é‡ã€‚

ç”¨æ³•ï¼š
    python editor.py <ä¹¦ç±æ ¹ç›®å½•> <å‘½ä»¤> [å‚æ•°]

å‘½ä»¤ï¼š
    proofread <èŠ‚å·>       - å¯¹æŒ‡å®šå°èŠ‚è¿›è¡Œæ ¡å¯¹æ£€æŸ¥
    terminology             - æœ¯è¯­ä¸€è‡´æ€§æ£€æŸ¥
    checklist               - ç”Ÿæˆå‡ºç‰ˆæ£€æŸ¥æ¸…å•
    preface                 - ç”Ÿæˆ/æ£€æŸ¥å‰è¨€ã€å†…å®¹ç®€ä»‹ç­‰è¾…æ–‡

ç¤ºä¾‹ï¼š
    python editor.py . proofread 1.2.1
    python editor.py . terminology
    python editor.py . checklist
"""

import sys
import re
from pathlib import Path
from dataclasses import dataclass
from typing import List, Tuple


# æœ¯è¯­è§„èŒƒæ£€æŸ¥è§„åˆ™
TERMINOLOGY_RULES = [
    # (æ¨¡å¼, æ­£ç¡®å½¢å¼, è¯´æ˜)
    (r'SKILL\.md', 'SKILL.md', 'æ–‡ä»¶ååº”å…¨å¤§å†™'),
    (r'^[Ss]kill(?!\.)', 'Skill', 'å¥ä¸­ Skill é¦–å­—æ¯å¤§å†™'),
    (r'claude(?!\s|[-])', 'Claude', 'Claude äº§å“åé¦–å­—æ¯å¤§å†™'),
    (r'anthropic(?!\s)', 'Anthropic', 'Anthropic å…¬å¸åé¦–å­—æ¯å¤§å†™'),
    (r'github', 'GitHub', 'GitHub H å¤§å†™'),
    (r'\bagent\b(?!\s[Ss]kill)', 'Agent', 'Agent ä¸“æœ‰åè¯é¦–å­—æ¯å¤§å†™'),
    (r'æ’ä»¶|è„šæœ¬|å®(?=.*[Ss]kill)', '[é¿å…æ··ç”¨]', 'ç»Ÿä¸€ç”¨ Skill/æŠ€èƒ½'),
    (r'æœ¬èŠ‚å°†ä»‹ç»|ç»¼ä¸Šæ‰€è¿°|ä¼—æ‰€å‘¨çŸ¥', '[ç¦ç”¨è¯]', 'é¿å…å­¦æœ¯è…”'),
    (r'ä¸€è¨€ä»¥è”½ä¹‹|æ¯‹åº¸ç½®ç–‘|æ˜¾è€Œæ˜“è§', '[ç¦ç”¨è¯]', 'é¿å…é™ˆè¯æ»¥è°ƒ'),
    (r'æ˜¾ç„¶|æ˜“å¾—|ç®€å•(?=.*è¯»è€…)', '[ç¦ç”¨è¯]', 'ä¸è¦å±…é«˜ä¸´ä¸‹'),
]


# é¦–æ¬¡å‡ºç°éœ€è§£é‡Šçš„æœ¯è¯­
TERMS_NEED_EXPLANATION = [
    'Agent Skill',
    'Progressive Disclosure',
    'MCP',
    'Subagent',
    'YAML Frontmatter',
    'Context Window',
    'Fork',
    'Hook',
]


@dataclass
class Issue:
    level: str  # ERROR, WARN, INFO
    message: str
    suggestion: str = ""


def parse_frontmatter(file_path: Path) -> Tuple[dict, str]:
    """è§£æ frontmatter å’Œæ­£æ–‡ã€‚"""
    content = file_path.read_text(encoding='utf-8')
    match = re.match(r'^---\s*\n(.*?)\n---\s*\n(.*)', content, re.DOTALL)

    if not match:
        return {}, content

    fm = {}
    for line in match.group(1).strip().split('\n'):
        if ':' in line:
            key, _, value = line.partition(':')
            fm[key.strip()] = value.strip().strip('"\'')

    return fm, match.group(2)


def check_terminology(file_path: Path) -> List[Issue]:
    """æ£€æŸ¥æœ¯è¯­ä¸€è‡´æ€§ã€‚"""
    issues = []
    content = file_path.read_text(encoding='utf-8')
    fm, body = parse_frontmatter(file_path)

    # æ£€æŸ¥ç¦ç”¨è¯
    forbidden_patterns = [
        (r'æœ¬èŠ‚å°†ä»‹ç»|ç»¼ä¸Šæ‰€è¿°|ä¼—æ‰€å‘¨çŸ¥', 'é¿å…å­¦æœ¯è…”ï¼Œç›´æ¥é™ˆè¿°'),
        (r'ä¸€è¨€ä»¥è”½ä¹‹|æ¯‹åº¸ç½®ç–‘|æ˜¾è€Œæ˜“è§', 'é¿å…é™ˆè¯æ»¥è°ƒ'),
        (r'æ˜¾ç„¶|æ˜“å¾—|ç®€å•', 'ä¸è¦å±…é«˜ä¸´ä¸‹ï¼Œå‡è®¾è¯»è€…æ‡‚'),
        (r'foo|bar|baz', 'ç”¨çœŸå®ç¤ºä¾‹ï¼Œé¿å…å ä½ç¬¦'),
    ]

    for pattern, suggestion in forbidden_patterns:
        matches = re.finditer(pattern, body)
        for match in matches:
            issues.append(Issue(
                level='WARN',
                message=f'å‘ç°ç¦ç”¨è¯: "{match.group()}"',
                suggestion=suggestion
            ))

    # æ£€æŸ¥å¤§å°å†™
    case_patterns = [
        (r'\bclaude\b(?!\s+(?:Code|4|3))', 'Claude', 'äº§å“åé¦–å­—æ¯å¤§å†™'),
        (r'\bgithub\b', 'GitHub', 'H å¤§å†™'),
        (r'\banthropic\b', 'Anthropic', 'å…¬å¸åé¦–å­—æ¯å¤§å†™'),
    ]

    for pattern, correct, desc in case_patterns:
        matches = re.finditer(pattern, body, re.IGNORECASE)
        for match in matches:
            if match.group() != correct:
                issues.append(Issue(
                    level='INFO',
                    message=f'å¤§å°å†™å»ºè®®: "{match.group()}" -> "{correct}"',
                    suggestion=desc
                ))

    # æ£€æŸ¥æœ¯è¯­é¦–æ¬¡å‡ºç°æ˜¯å¦æœ‰è§£é‡Š
    explained_terms = set()
    for term in TERMS_NEED_EXPLANATION:
        # æŸ¥æ‰¾é¦–æ¬¡å‡ºç°
        match = re.search(rf'\b{re.escape(term)}\b', body, re.IGNORECASE)
        if match:
            # æ£€æŸ¥å‰åæ˜¯å¦æœ‰ä¸­æ–‡è§£é‡Š
            start = max(0, match.start() - 50)
            end = min(len(body), match.end() + 50)
            context = body[start:end]

            # ç®€å•åˆ¤æ–­æ˜¯å¦æœ‰ä¸­æ–‡æ‹¬å·æˆ–"ç®€ç§°"ç­‰è¯
            has_chinese = bool(re.search(r'[\u4e00-\u9fff]', context))
            has_parens = '(' in context and ')' in context

            if not (has_chinese and has_parens):
                issues.append(Issue(
                    level='WARN',
                    message=f'æœ¯è¯­é¦–æ¬¡å‡ºç°å»ºè®®è§£é‡Š: "{term}"',
                    suggestion='æ·»åŠ ä¸­æ–‡è§£é‡Šï¼Œå¦‚: Agent Skill (æ™ºèƒ½ä½“æŠ€èƒ½)'
                ))

    return issues


def check_formatting(file_path: Path) -> List[Issue]:
    """æ£€æŸ¥æ ¼å¼é—®é¢˜ã€‚"""
    issues = []
    content = file_path.read_text(encoding='utf-8')
    fm, body = parse_frontmatter(file_path)

    # æ£€æŸ¥ä»£ç å—
    code_blocks = re.findall(r'```(\w+)?', body)
    for i, lang in enumerate(code_blocks):
        if not lang:
            issues.append(Issue(
                level='INFO',
                message=f'ä»£ç å— #{i+1} æœªæŒ‡å®šè¯­è¨€',
                suggestion='æ·»åŠ è¯­è¨€æ ‡è®°å¦‚ ```python ```bash'
            ))

    # æ£€æŸ¥æ®µè½é•¿åº¦
    paragraphs = body.split('\n\n')
    for i, para in enumerate(paragraphs):
        lines = para.strip().split('\n')
        if len(lines) > 8 and not para.startswith('```'):
            issues.append(Issue(
                level='INFO',
                message=f'ç¬¬ {i+1} æ®µè½è¾ƒé•¿ ({len(lines)} è¡Œ)',
                suggestion='å»ºè®®æ‹†åˆ†ä¸ºçŸ­æ®µè½ï¼Œä¾¿äºé˜…è¯»'
            ))

    # æ£€æŸ¥æ ‡é¢˜å±‚çº§
    headings = re.findall(r'^(#{1,6})\s+', body, re.MULTILINE)
    prev_level = 0
    for heading in headings:
        level = len(heading)
        if level > prev_level + 1 and prev_level > 0:
            issues.append(Issue(
                level='WARN',
                message=f'æ ‡é¢˜å±‚çº§è·³è·ƒ: {prev_level} -> {level}',
                suggestion='æ ‡é¢˜å±‚çº§åº”é€çº§é€’è¿›'
            ))
        prev_level = level

    return issues


def check_content_quality(file_path: Path) -> List[Issue]:
    """æ£€æŸ¥å†…å®¹è´¨é‡ã€‚"""
    issues = []
    fm, body = parse_frontmatter(file_path)

    # ç»Ÿè®¡å­—æ•°
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', body))
    english_words = len(re.findall(r'[a-zA-Z]+', body))
    total_words = chinese_chars + english_words

    target = int(fm.get('target_words', 0))
    if target > 0:
        ratio = total_words / target
        if ratio < 0.5:
            issues.append(Issue(
                level='ERROR',
                message=f'å­—æ•°ä¸¥é‡ä¸è¶³: {total_words}/{target} ({ratio:.0%})',
                suggestion='å†…å®¹éœ€è¦å¤§å¹…æ‰©å……'
            ))
        elif ratio < 0.8:
            issues.append(Issue(
                level='WARN',
                message=f'å­—æ•°åå°‘: {total_words}/{target} ({ratio:.0%})',
                suggestion='å¯ä»¥é€‚å½“å¢åŠ å†…å®¹æ·±åº¦æˆ–æ¡ˆä¾‹'
            ))

    # æ£€æŸ¥ä»£ç ç¤ºä¾‹
    code_blocks = len(re.findall(r'```', body)) // 2
    if code_blocks == 0 and chinese_chars > 500:
        issues.append(Issue(
            level='INFO',
            message='æœªæ£€æµ‹åˆ°ä»£ç ç¤ºä¾‹',
            suggestion='æŠ€æœ¯ä¹¦ç±å»ºè®®æ·»åŠ å¯è¿è¡Œçš„ä»£ç ç¤ºä¾‹'
        ))

    # æ£€æŸ¥å›¾è¡¨
    has_table = '|' in body and '---' in body
    has_diagram = 'mermaid' in body or '![' in body
    if not has_table and not has_diagram and chinese_chars > 1000:
        issues.append(Issue(
            level='INFO',
            message='å»ºè®®æ·»åŠ å›¾è¡¨è¾…åŠ©è¯´æ˜',
            suggestion='é•¿æ®µè½å¯ç”¨è¡¨æ ¼æˆ–å›¾è¡¨å¢å¼ºå¯è¯»æ€§'
        ))

    return issues


def cmd_proofread(book_dir: Path, section_id: str):
    """æ ¡å¯¹æŒ‡å®šå°èŠ‚ã€‚"""
    # æŸ¥æ‰¾å°èŠ‚æ–‡ä»¶
    chapter_dirs = [
        'å¼•è¨€',
        'ç¬¬ä¸€ç« _è®¤è¯†Agent_Skill',
        'ç¬¬äºŒç« _Skillçš„åˆ†ç±»ä¸ç”Ÿæ€',
        'ç¬¬ä¸‰ç« _Agent_Skillå¼€å‘å®æˆ˜',
    ]

    target_file = None
    for chapter_dir in chapter_dirs:
        chapter_path = book_dir / chapter_dir
        if not chapter_path.exists():
            continue

        for md_file in chapter_path.glob('*.md'):
            if md_file.name.endswith('_å®Œæ•´.md'):
                continue

            fm, _ = parse_frontmatter(md_file)
            if fm.get('section_id') == section_id:
                target_file = md_file
                break

        if target_file:
            break

    if not target_file:
        print(f'âŒ æœªæ‰¾åˆ°å°èŠ‚: {section_id}')
        return

    print(f'ğŸ” æ­£åœ¨æ ¡å¯¹: {target_file.name}')
    print('=' * 60)

    all_issues = []
    all_issues.extend(check_terminology(target_file))
    all_issues.extend(check_formatting(target_file))
    all_issues.extend(check_content_quality(target_file))

    if not all_issues:
        print('âœ… æ ¡å¯¹é€šè¿‡ï¼Œæœªå‘ç°é—®é¢˜')
        return

    # åˆ†çº§æ˜¾ç¤º
    errors = [i for i in all_issues if i.level == 'ERROR']
    warns = [i for i in all_issues if i.level == 'WARN']
    infos = [i for i in all_issues if i.level == 'INFO']

    if errors:
        print(f'\nâŒ é”™è¯¯ ({len(errors)}):')
        for issue in errors:
            print(f'  - {issue.message}')
            if issue.suggestion:
                print(f'    å»ºè®®: {issue.suggestion}')

    if warns:
        print(f'\nâš ï¸ è­¦å‘Š ({len(warns)}):')
        for issue in warns:
            print(f'  - {issue.message}')
            if issue.suggestion:
                print(f'    å»ºè®®: {issue.suggestion}')

    if infos:
        print(f'\nâ„¹ï¸ æç¤º ({len(infos)}):')
        for issue in infos:
            print(f'  - {issue.message}')
            if issue.suggestion:
                print(f'    å»ºè®®: {issue.suggestion}')

    print(f'\n{"=" * 60}')
    print(f'æ€»è®¡: {len(errors)} é”™è¯¯ | {len(warns)} è­¦å‘Š | {len(infos)} æç¤º')


def cmd_terminology(book_dir: Path):
    """å…¨ä¹¦æœ¯è¯­æ£€æŸ¥ã€‚"""
    print('ğŸ“š æœ¯è¯­ä¸€è‡´æ€§æ£€æŸ¥')
    print('=' * 60)

    glossary_path = book_dir / '.claude' / 'skills' / 'book-writer' / 'references' / 'glossary.md'
    if glossary_path.exists():
        print('âœ… æœ¯è¯­è¡¨å·²å­˜åœ¨')
        print(f'   è·¯å¾„: {glossary_path}')
    else:
        print('âš ï¸ æœ¯è¯­è¡¨ä¸å­˜åœ¨')
        print('   å»ºè®®åˆ›å»º: references/glossary.md')

    print('\néœ€è¦å…¨æ–‡ç»Ÿä¸€çš„æœ¯è¯­:')
    for term in TERMS_NEED_EXPLANATION:
        print(f'  - {term}')


def cmd_checklist(book_dir: Path):
    """ç”Ÿæˆå‡ºç‰ˆæ£€æŸ¥æ¸…å•ã€‚"""
    checklist_path = book_dir / '.claude' / 'skills' / 'book-writer' / 'references' / 'editor-checklist.md'

    if checklist_path.exists():
        print('ğŸ“‹ å‡ºç‰ˆæ£€æŸ¥æ¸…å•')
        print('=' * 60)
        print(f'å·²å­˜åœ¨: {checklist_path}')
        print('\nåŒ…å«æ£€æŸ¥é¡¹:')
        print('  - æ”¿æ²»å¯¼å‘ä¸åˆè§„')
        print('  - ç‰ˆæƒå®¡æŸ¥')
        print('  - äº‹å®æ ¸æŸ¥')
        print('  - ç¼–æ ¡è´¨é‡')
        print('  - ä½“ä¾‹æ ¼å¼')
        print('  - è£…å¸§è®¾è®¡')
        print('  - å°åˆ·å‰æ£€æŸ¥')
    else:
        print('âš ï¸ æ£€æŸ¥æ¸…å•ä¸å­˜åœ¨')


def main():
    if len(sys.argv) < 3:
        print(__doc__)
        sys.exit(1)

    book_dir = Path(sys.argv[1])
    command = sys.argv[2]

    if not book_dir.exists():
        print(f'é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {book_dir}')
        sys.exit(1)

    if command == 'proofread':
        if len(sys.argv) < 4:
            print('ç”¨æ³•: editor.py <ç›®å½•> proofread <èŠ‚å·>')
            sys.exit(1)
        cmd_proofread(book_dir, sys.argv[3])
    elif command == 'terminology':
        cmd_terminology(book_dir)
    elif command == 'checklist':
        cmd_checklist(book_dir)
    else:
        print(f'æœªçŸ¥å‘½ä»¤: {command}')
        print(__doc__)
        sys.exit(1)


if __name__ == '__main__':
    main()
