---
section_id: 10.5
title: 系统监控与告警
status: draft
target_words: 1500
word_count: 1620
---

# 系统监控与告警

系统稳定性是业务连续性的基础。随着系统规模的增长，监控的复杂度也在不断增加：海量的监控指标、频繁的告警噪音、难以定位的根因。OpenClaw 可以帮助我们构建一个智能监控助手，实现异常自动检测、告警智能降噪、根因自动分析等功能，让运维工作更加高效和智能化。

## 场景介绍

### 系统监控的挑战

1. **指标爆炸**：微服务架构下，监控指标呈指数级增长
2. **告警疲劳**：大量低价值告警淹没真正重要的问题
3. **根因难定位**：故障往往涉及多个系统，定位耗时
4. **响应滞后**：人工响应无法满足高可用要求
5. **预测困难**：难以提前发现潜在问题

### OpenClaw 的解决方案

- **智能降噪**：过滤无效告警，聚合相关问题
- **异常检测**：基于AI自动识别异常模式
- **根因分析**：自动分析故障传播链，定位根因
- **预测告警**：提前预警潜在问题
- **自动响应**：对已知问题自动执行修复操作

## 实现思路

智能监控系统的核心组件包括：

1. **指标收集器**：从各类监控系统收集指标数据
2. **异常检测器**：使用机器学习识别异常模式
3. **告警处理器**：过滤、聚合、关联告警
4. **根因分析器**：分析故障传播路径
5. **自动响应器**：执行预定义的修复操作

## 配置步骤

### 第一步：创建监控集成 Skill

```yaml
# ~/.openclaw/skills/monitoring/SKILL.md
---
name: monitoring-assistant
description: 系统监控助手
commands:
  - name: fetch-metrics
    description: 获取监控指标
    script: |
      # 从 Prometheus 查询指标
      curl -s "http://{{prometheus_url}}/api/v1/query?query={{query}}&time={{timestamp}}" | \
        jq '.data.result | map({
          metric: .metric,
          value: .value[1]
        })'

  - name: fetch-logs
    description: 获取日志数据
    script: |
      # 从 Elasticsearch 查询日志
      curl -s -X POST "http://{{es_url}}/logs-*/_search" \
        -H "Content-Type: application/json" \
        -d '{
          "query": {
            "bool": {
              "must": [
                {"match": {"service": "{{service}}"}},
                {"range": {"@timestamp": {"gte": "{{start_time}}", "lte": "{{end_time}}"}}}
              ]
            }
          },
          "size": 100,
          "sort": [{"@timestamp": "desc"}]
        }' | jq '.hits.hits | map(._source)'

  - name: fetch-traces
    description: 获取链路追踪数据
    script: |
      # 从 Jaeger 查询追踪
      curl -s "http://{{jaeger_url}}/api/traces?service={{service}}&start={{start_time}}&end={{end_time}}&limit=100" | \
        jq '.data | map({
          traceID: .traceID,
          duration: .duration,
          spans: [.spans | map({operationName: .operationName, duration: .duration, tags: .tags})]
        })'

  - name: send-alert
    description: 发送告警通知
    script: |
      curl -s -X POST "{{webhook_url}}" \
        -H "Content-Type: application/json" \
        -d '{
          "title": "{{alert_title}}",
          "severity": "{{severity}}",
          "message": "{{alert_message}}",
          "metadata": {{metadata | to_json}}
        }'

  - name: execute-remediation
    description: 执行修复操作
    script: |
      # 执行修复脚本
      {{remediation_script}}
```

### 第二步：创建监控分析 Agent

```yaml
# ~/.openclaw/agents/monitoring-analyst.yaml
name: monitoring-analyst
description: 系统监控分析专家
system_prompt: |
  你是一位资深的SRE工程师，擅长系统监控、故障排查和性能优化。

  分析能力：
  1. 指标异常检测
  2. 日志模式识别
  3. 链路追踪分析
  4. 根因定位
  5. 容量规划

  告警处理原则：
  - 区分症状和根因
  - 识别告警关联性
  - 评估业务影响
  - 提供可操作的建议

  输出格式（JSON）：
  {
    "anomaly_detected": true,
    "severity": "critical|high|medium|low",
    "affected_services": ["service1", "service2"],
    "root_cause_analysis": "根因分析",
    "impact_assessment": "影响评估",
    "recommended_actions": ["建议操作1", "建议操作2"],
    "auto_remediation_possible": true,
    "confidence": 0.85
  }
```

### 第三步：编写监控工作流

```yaml
# ~/.openclaw/workflows/monitoring.lobster
name: monitoring
version: "1.0"

triggers:
  - type: webhook
    path: "/alertmanager/webhook"
  - type: schedule
    cron: "*/5 * * * *"  # 每5分钟检查一次

steps:
  # 1. 收集监控数据
  - name: collect_metrics
    skill: monitoring-assistant
    command: fetch-metrics
    args:
      prometheus_url: "{{config.prometheus_url}}"
      query: "{{input.alert.query}}"

  - name: collect_logs
    skill: monitoring-assistant
    command: fetch-logs
    args:
      es_url: "{{config.elasticsearch_url}}"
      service: "{{input.alert.service}}"
      start_time: "{{now | subtract: '30m' | format_timestamp}}"
      end_time: "{{now | format_timestamp}}"

  - name: collect_traces
    skill: monitoring-assistant
    command: fetch-traces
    args:
      jaeger_url: "{{config.jaeger_url}}"
      service: "{{input.alert.service}}"
      start_time: "{{now | subtract: '30m' | format_timestamp}}"
      end_time: "{{now | format_timestamp}}"

  # 2. 分析异常
  - name: analyze_anomaly
    agent: monitoring-analyst
    prompt: |
      分析以下监控数据，识别异常和根因：

      告警信息：
      {{input.alert}}

      指标数据：
      {{steps.collect_metrics.output}}

      相关日志：
      {{steps.collect_logs.output}}

      链路追踪：
      {{steps.collect_traces.output}}

      历史模式：
      {{monitoring.history_patterns}}

      请提供详细的分析报告。

  # 3. 检查告警关联性
  - name: check_correlation
    skill: monitoring-assistant
    command: query-active-alerts
    args:
      time_window: "15m"

  - name: correlate_alerts
    agent: monitoring-analyst
    prompt: |
      分析当前告警与以下活跃告警的关联性：
      当前告警：{{input.alert}}
      活跃告警：{{steps.check_correlation.output}}

      识别：
      1. 是否同一根因导致
      2. 告警传播路径
      3. 主告警和衍生告警

  # 4. 评估自动修复可行性
  - name: assess_auto_remediation
    agent: monitoring-analyst
    prompt: |
      基于以下分析结果，评估自动修复可行性：
      {{steps.analyze_anomaly.output}}

      已知修复方案：
      {{remediation.playbooks}}

      请判断：
      1. 是否匹配已知问题模式
      2. 自动修复的风险评估
      3. 建议的操作

  # 5. 执行自动修复（如可行）
  - name: auto_remediate
    skill: monitoring-assistant
    command: execute-remediation
    condition: "{{steps.assess_auto_remediation.output.auto_remediation_safe}}"
    args:
      remediation_script: "{{steps.assess_auto_remediation.output.recommended_script}}"

  # 6. 发送智能告警
  - name: send_smart_alert
    skill: monitoring-assistant
    command: send-alert
    args:
      webhook_url: "{{config.alert_webhook}}"
      alert_title: "[{{steps.analyze_anomaly.output.severity}}] {{input.alert.title}}"
      severity: "{{steps.analyze_anomaly.output.severity}}"
      alert_message: |
        ## 异常摘要
        {{steps.analyze_anomaly.output.summary}}

        ## 根因分析
        {{steps.analyze_anomaly.output.root_cause_analysis}}

        ## 影响范围
        {{steps.analyze_anomaly.output.impact_assessment}}

        ## 建议操作
        {{steps.analyze_anomaly.output.recommended_actions}}

        {% if steps.auto_remediate.status == 'executed' %}
        ## 自动修复
        已执行自动修复：{{steps.auto_remediate.output.result}}
        {% endif %}
      metadata:
        alert_id: "{{input.alert.id}}"
        correlation_group: "{{steps.correlate_alerts.output.group_id}}"
        confidence: "{{steps.analyze_anomaly.output.confidence}}"

  # 7. 记录事件
  - name: record_incident
    skill: monitoring-assistant
    command: create-incident
    condition: "{{steps.analyze_anomaly.output.severity in ['critical', 'high']}}"
    args:
      title: "{{input.alert.title}}"
      severity: "{{steps.analyze_anomaly.output.severity}}"
      analysis: "{{steps.analyze_anomaly.output}}"
      affected_services: "{{steps.analyze_anomaly.output.affected_services}}"
```

### 第四步：配置监控策略

```yaml
# ~/.openclaw/config.yaml
monitoring:
  data_sources:
    prometheus:
      url: "http://prometheus:9090"
    elasticsearch:
      url: "http://elasticsearch:9200"
    jaeger:
      url: "http://jaeger:16686"

  anomaly_detection:
    enabled: true
    algorithms:
      - name: "statistical"
        threshold: 3  # 3-sigma
      - name: "seasonal"
        period: "1d"  # 日周期
      - name: "trend"
        sensitivity: 0.1

  alert_processing:
    deduplication_window: "15m"
    correlation_window: "30m"
    suppression_rules:
      - name: "maintenance_window"
        condition: "{{now | is_maintenance_window}}"
        action: "suppress"
      - name: "flapping"
        condition: "{{alert.count_15m > 3}}"
        action: "throttle"

  auto_remediation:
    enabled: true
    playbooks:
      - name: "restart_service"
        condition: "{{alert.name == 'service_down' and alert.duration > 2m}}"
        action: "kubectl rollout restart deployment/{{service}}"
        approval_required: false
      - name: "scale_up"
        condition: "{{alert.name == 'high_cpu' and alert.value > 90}}"
        action: "kubectl scale deployment/{{service}} --replicas={{current + 2}}"
        approval_required: true

  notifications:
    critical:
      channels: ["pagerduty", "slack", "sms"]
      escalation: "5m"
    high:
      channels: ["slack", "email"]
    medium:
      channels: ["slack"]
    low:
      channels: []
      digest: "daily"
```

## 实际案例

### 案例一：智能告警降噪

运维团队使用 OpenClaw 处理告警风暴：

```yaml
# 告警风暴场景
input:
  alerts:
    - name: "api_gateway_high_latency"
      service: "api-gateway"
      severity: "critical"
    - name: "auth_service_timeout"
      service: "auth-service"
      severity: "critical"
    - name: "user_db_connection_slow"
      service: "user-db"
      severity: "high"
    - name: "cache_hit_rate_low"
      service: "redis-cache"
      severity: "medium"

# 工作流执行结果
steps:
  correlate_alerts:
    output: |
      告警关联分析：

      识别到相关告警组：
      - 根因：user-db 连接池耗尽
      - 影响路径：
        user-db (慢查询)
          → auth-service (认证超时)
            → api-gateway (高延迟)
      - 衍生告警：cache_hit_rate_low (缓存穿透)

      建议：
      1. 优先处理 user-db 连接池问题
      2. 临时扩容 auth-service
      3. 其他告警可暂缓处理

  send_smart_alert:
    output: |
      发送聚合告警（替代4条独立告警）：

      标题：[CRITICAL] 数据库连接池耗尽导致服务链延迟

      根因：user-db 连接池耗尽（当前连接数：100/100）
      影响：auth-service、api-gateway 响应延迟
      建议：立即检查慢查询，临时扩容连接池
```

### 案例二：异常预测

SRE 团队使用 OpenClaw 提前发现潜在问题：

```yaml
name: predictive-monitoring

triggers:
  - type: schedule
    cron: "0 */6 * * *"  # 每6小时分析一次

steps:
  - name: fetch_historical_metrics
    skill: monitoring-assistant
    command: fetch-metrics
    args:
      query: |
        rate(http_requests_total[5m])
        / ignoring(instance) group_left
        avg_over_time(http_requests_total[1d])

  - name: detect_patterns
    agent: monitoring-analyst
    prompt: |
      分析以下历史指标，识别潜在风险：
      {{steps.fetch_historical_metrics.output}}

      请识别：
      1. 资源使用趋势（增长/下降/波动）
      2. 异常模式（周期性、突发性）
      3. 容量瓶颈预警
      4. 预测未来7天的问题

  - name: generate_predictions
    agent: monitoring-analyst
    prompt: |
      基于趋势分析，生成预测报告：
      {{steps.detect_patterns.output}}

      请提供：
      1. 预计何时达到容量上限
      2. 建议的扩容时间点
      3. 成本优化建议

  - name: send_prediction_report
    skill: notification
    command: send
    args:
      channel: "email"
      to: "sre@company.com"
      title: "容量预测报告 - {{date}}"
      content: "{{steps.generate_predictions.output}}"
```

### 案例三：故障自动恢复

系统使用 OpenClaw 自动处理常见故障：

```yaml
name: auto-recovery

triggers:
  - type: webhook
    path: "/alert/auto-recovery"

steps:
  - name: classify_issue
    agent: monitoring-analyst
    prompt: |
      分类以下故障类型：
      {{input.alert}}

      匹配已知故障模式：
      {{recovery.playbooks}}

  - name: execute_recovery
    skill: monitoring-assistant
    command: execute-remediation
    condition: "{{steps.classify_issue.output.match_found}}"
    args:
      remediation_script: "{{steps.classify_issue.output.playbook.action}}"

  - name: verify_recovery
    skill: monitoring-assistant
    command: fetch-metrics
    args:
      query: "{{input.alert.metric}}"
      wait_for: "{{steps.classify_issue.output.playbook.verify_condition}}"
      timeout: "5m"

  - name: notify_result
    skill: notification
    command: send
    args:
      channel: "slack"
      recipient: "#sre-alerts"
      title: "自动恢复{{steps.verify_recovery.output.success ? '成功' : '失败'}}"
      content: |
        故障：{{input.alert.name}}
        服务：{{input.alert.service}}
        执行操作：{{steps.classify_issue.output.playbook.name}}
        恢复结果：{{steps.verify_recovery.output.result}}

        {% if not steps.verify_recovery.output.success %}
        需要人工介入！
        {% endif %}
```

## 进阶技巧

### 混沌工程集成

```yaml
- name: chaos_experiment
  skill: monitoring-assistant
  command: run-chaos
  args:
    experiment: "pod-failure"
    target: "{{service}}"
    duration: "10m"

- name: monitor_resilience
  agent: monitoring-analyst
  prompt: |
    监控混沌实验期间的系统表现：
    实验：{{steps.chaos_experiment.output}}
    指标：{{metrics.during_experiment}}

    评估系统的韧性表现。
```

### 成本优化分析

```yaml
- name: analyze_resource_usage
  skill: monitoring-assistant
  command: fetch-metrics
  args:
    query: "container_resource_usage"

- name: identify_optimization
  agent: monitoring-analyst
  prompt: |
    分析资源使用数据，识别优化机会：
    {{steps.analyze_resource_usage.output}}

    请识别：
    1. 资源利用率过低的服务
    2. 过度配置的资源
    3. 可优化的成本点
```

通过这些配置，OpenClaw 可以成为你的智能监控助手，帮助团队实现更高效、更智能的运维管理。
